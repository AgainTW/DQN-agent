# -*- coding: utf-8 -*-
"""ag_main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IyHp0rvKKYas32NSrBqYd0obwfruR11J

# Import packages
"""

import os
import sys
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.optimizers import RMSprop

"""# Coordinate Q learning

## build Q learning
"""

# 實際進行訓練的 evaluation network
class Eval_Q_Model(tf.keras.Model):
  # Evaluate Q Model 的模型架構
  def __init__(self, num_actions):
    super().__init__('mlp_q_network')
    self.layer1 = layers.Dense(10, activation='relu')
    self.logits = layers.Dense(num_actions, activation=None)

  def call(self, inputs):
    x = tf.convert_to_tensor(inputs)
    layer1 = self.layer1(x)
    logits = self.logits(layer1)
    return logits

# 久久更新一次的 evaluation network
class Target_Q_Model(tf.keras.Model):
  # Target Q Model 的模型架構(不更新layer權重)
  def __init__(self, num_actions):
    super().__init__('mlp_q_network_1')
    self.layer1 = layers.Dense(10, trainable=False, activation='relu')
    self.logits = layers.Dense(num_actions, trainable=False, activation=None)

  def call(self, inputs):
    x = tf.convert_to_tensor(inputs)
    layer1 = self.layer1(x)
    logits = self.logits(layer1)
    return logits

class DeepQNetwork:
  def __init__(self, n_actions, n_features, eval_model, target_model):
    # 利用dic儲存參數
    self.params = {
        'n_actions': n_actions,     # actions數量
        'n_features': n_features,    # features數量 
        'learning_rate': 0.01,
        'reward_decay': 0.9,
        'e_greedy': 0.9,         # 選擇隨機action的機率
        'replace_target_iter': 300,   # 從 Eval_Q_Model 更新 Target_Q_Model 需要的迭代次數
        'memory_size': 500,
        'batch_size': 32,
        'e_greedy_increment': None
        }

    # total learning step
    self.learn_step_counter = 0
    # epsilon 表機率，訓練過程中有 epsilon 的機率 agent 會選擇隨機
    self.epsilon = 0 if self.params['e_greedy_increment'] is not None else self.params['e_greedy']
    # initialize zero memory ，每個 memory 中的 experience 大小為 (state + next state + reward + action)
    self.memory = np.zeros((self.params['memory_size'], self.params['n_features'] * 2 + 2))

    self.eval_model = eval_model
    self.target_model = target_model

    self.eval_model.compile(
        optimizer=RMSprop(learning_rate=self.params['learning_rate']),
        loss='mse'
        )
    self.cost_his = []

  ## 儲存經驗
  def store_transition(self, s, a, r, s_):
    if not hasattr(self, 'memory_counter'):
      self.memory_counter = 0

    # 打包agent經驗
    transition = np.hstack((s, [a, r], s_))

    # 用新memory取代舊memory
    index = self.memory_counter % self.params['memory_size']
    self.memory[index, :] = transition
    self.memory_counter += 1

  ## 決定是否具有隨機以及隨機的方式
  def choose_action(self, observation):
    # to have batch dimension when feed into tf placeholder
    observation = observation[np.newaxis, :]

    if np.random.uniform() < self.epsilon:
    # 透過觀察得到每個 action 的 q 值
      actions_value = self.eval_model.predict(observation)
      print(actions_value)
      action = np.argmax(actions_value)
    else: # 隨機移動
      action = np.random.randint(0, self.params['n_actions'])
    return action

  ## 從 memory 中取樣學習
  def learn(self):
    # 從 memory 取樣 batch memory
    if self.memory_counter > self.params['memory_size']:
      sample_index = np.random.choice(self.params['memory_size'], size=self.params['batch_size'])
    else:
      sample_index = np.random.choice(self.memory_counter, size=self.params['batch_size'])

    batch_memory = self.memory[sample_index, :]

    # 計算現在 eval net 和 target net 得出 Q value 的落差
    q_next = self.target_model.predict(batch_memory[:, -self.params['n_features']:])
    q_eval = self.eval_model.predict(batch_memory[:, :self.params['n_features']])

    # 根據 q_eval 改變 q_target
    q_target = q_eval.copy()

    batch_index = np.arange(self.params['batch_size'], dtype=np.int32)
    eval_act_index = batch_memory[:, self.params['n_features']].astype(int)
    reward = batch_memory[:, self.params['n_features'] + 1]

    q_target[batch_index, eval_act_index] = reward + self.params['reward_decay'] * np.max(q_next, axis=1)

    # check to replace target parameters
    if self.learn_step_counter % self.params['replace_target_iter'] == 0:
      for eval_layer, target_layer in zip(self.eval_model.layers, self.target_model.layers):
        target_layer.set_weights(eval_layer.get_weights())
      print('\ntarget_params_replaced\n')

    # train eval network
    self.cost = self.eval_model.train_on_batch(batch_memory[:, :self.params['n_features']], q_target)
    self.cost_his.append(self.cost)

    # 增加 epsilon ，使 agent 傾向不隨機
    self.epsilon = self.epsilon + self.params['e_greedy_increment'] if self.epsilon < self.params['e_greedy'] else self.params['e_greedy']
    self.learn_step_counter += 1

  def plot_cost(self):
    plt.plot(np.arange(len(self.cost_his)), self.cost_his)
    plt.ylabel('Cost')
    plt.xlabel('training steps')
    plt.show()

def run_cleaner(ag, RL):
  step = 0

  action = 1
  n_states = ag.get_env()
  error_flag, reward, done_flag = ag.step(action)
  n_states_new = ag.get_env()
  RL.store_transition(n_states, action, reward, n_states_new)

  for episode in range(300):
    # RL 選擇 action
    action = RL.choose_action(n_states_new)
    # 執行 action 並得到下一階段 info
    n_states = n_states_new
    error_flag, reward, done_flag = ag.step(action)
    n_states_new = ag.get_env()
    RL.store_transition(n_states, action, reward, n_states_new)

    if (step > 200) and (step % 5 == 0):
      RL.learn()

    # break while loop when end of this episode
    if done_flag:
      print('Episode finished after {} timesteps, total rewards {}'.format(step+1, rewards))
      break
    step += 1
  # end of game
  print('over')

"""## build enviroment"""

class enviroment:
	def __init__(self):
		self.env = []
		self.wall = []

	def initial(self, setini):
		#	setini[0]：環境長	setini[1]：環境寬
		self.env = np.full((setini[0]+2, setini[1]+2), False, dtype=bool)
		self.wall = np.full((setini[0]+2, setini[1]+2), False, dtype=bool)

		#	包邊 w
		for j in range(setini[1]+2):
			self.wall[0][j] = 1
			self.wall[setini[0]+1][j] = 1
		for i in range(setini[0]+2):
			self.wall[i][0] = 1
			self.wall[i][setini[1]+1] = 1

	def set_wall(self, wall):
		for i in wall:
			if( i[1]>=np.size(self.env,1) or i[0]>=np.size(self.env,0) ): print("error",i)
			else:	self.wall[ i[1]+1 ][ i[0]+1 ] = 1

	def play(self):
		for i in range(np.size(self.env,0)):
			for j in range(np.size(self.env,1)):
				if(self.wall[i][j]):	print("w", end=" ")
				elif(self.env[i][j]):	print("■", end=" ")
				else:	print("□", end=" ")
			print("")

def agent_play_env(x, y, env):
	x_bound = np.size(env.env,1)
	y_bound = np.size(env.env,0)
	for i in range(-1,2,1):
		for j in range(-1,2,1):
			if( (j+x+1)>=x_bound or (i+y+1)>=y_bound ): print("error",i+x,j+j, end=" ")
			elif(i==0 and j==0):	print("a", end=" ")
			elif(env.wall[y+i+1][x+j+1]):	print("w", end=" ")
			elif(env.env[y+i+1][x+j+1]):	print("■", end=" ")
			else:	print("□", end=" ")
		print("")

def agent_play_env_full(x, y, env):
  print(x,y)
  x_bound = np.size(env.env,1)
  y_bound = np.size(env.env,0)
  for i in range(np.size(env.env,0)):
    for j in range(np.size(env.env,1)):
      if(i==y+1 and j==x+1):	print("a", end=" ")
      elif(env.wall[i][j]):	print("w", end=" ")
      elif(env.env[i][j]):	print("■", end=" ")
      else:	print("□", end=" ")
    print("")

def agent_play_env(x, y, env):
  x_bound = np.size(env.env,1)
  y_bound = np.size(env.env,0)
  for i in range(-1,2,1):
    for j in range(-1,2,1):
      if( (j+x+1)>=x_bound or (i+y+1)>=y_bound ): print("error",i+x,j+j, end=" ")
      elif(i==0 and j==0):  print("a", end=" ")
      elif(env.wall[y+i+1][x+j+1]):	print("w", end=" ")
      elif(env.env[y+i+1][x+j+1]):  print("■", end=" ")
      else:	print("□", end=" ")
    print("")

def agent_get_env(x, y, env):
  state =  np.zeros(9)
  for i in range(-1,2,1):
    for j in range(-1,2,1):
      if(i==0 and j==0):  pass
      elif(env.wall[y+i+1][x+j+1]): state[(j+1)+3*(i+1)] = 1
      elif(env.env[y+i+1][x+j+1]):  state[(j+1)+3*(i+1)] = 2
      else:	state[(j+1)+3*(i+1)] = 0
  return state

def agent_wall_test(x, y, env):
  x_bound = np.size(env.env,1)
  y_bound = np.size(env.env,0)
  if( (x+1)>=x_bound or (y+1)>=y_bound ): return 2
  elif(env.wall[y+1][x+1]==1):	return 1
  else:	return 0

class agent():
  def __init__(self):
    self.locx = -1
    self.locy = -1
    self.flag = 0
    self.bound_error = 0
    self.n_actions = 2
    self.n_features = np.zeros(9)
    self.env = 0

  def initial(self, env):
    self.env = env
    self.locx = 0
    self.locy = 0

  def set_loc(self, y, x):
    flag = agent_wall_test(x, y, self.env)
    if(flag==1):  print("wall_error")
    elif(flag==2):  print("bound_error")
    else:
      self.locx = x
      self.locy = y

  def loc_test(self, y, x):
    flag = agent_wall_test(x, y, self.env)
    if(flag==1):
      print("wall_error")
    elif(flag==2):	
      print("bound_error")
    return flag

  def go_right(self):
    self.locx += 1

  def go_left(self):
    self.locx -= 1

  def go_up(self):
    self.locy -= 1

  def go_down(self):
    self.locy += 1

  def go(self,do):
    if(do==0):	
      flag = self.loc_test(self.locy-1, self.locx)
      if(flag==0):  self.go_up()
    elif(do==1):
      flag = self.loc_test(self.locy+1, self.locx)
      if(flag==0):  self.go_down()
    elif(do==2):
      flag = self.loc_test(self.locy, self.locx-1)
      if(flag==0):  self.go_left()
    elif(do==3):
      flag = self.loc_test(self.locy, self.locx+1)
      if(flag==0):  self.go_right()
    self.flag = flag

  def get_env(self):
    return agent_get_env(self.locx, self.locy, self.env)

  def step(self,action):
    self.go(action)
    done = False

    # reward設計
    if(self.flag==1): reward = -1
    elif(self.flag==2):
      self.wall_error += 1
      reward = -1*(self.wall_error if self.wall_error<4 else 4)
    else: reward = 0.1 

    # 取得下一階段state
    state = agent_get_env(self.locx, self.locy, self.env)

    # 是否end game
    if(self.bound_error>10):  done = True

    return self.flag, reward, done

  def play(self):
    print("Location：", self.locx, self.locy)
    agent_play_env(self.locx, self.locy, self.env)

  def full_play(self):
    print("Location：", self.locx, self.locy)
    agent_play_env_full(self.locx, self.locy, self.env)

"""## main"""

# 環境初始化
env = enviroment()
env.initial([10,10])
wall = np.array( [ [5, 0], [5, 1] ] )
env.set_wall( wall )

# agent初始化
ag = agent()
ag.initial(env)
ag.set_loc(0, 0)

eval_model = Eval_Q_Model(num_actions=4)
target_model = Target_Q_Model(num_actions=4)
RL = DeepQNetwork(4, 9, eval_model, target_model)
for i in range(10):
  run_cleaner(ag, RL)

RL.plot_cost()