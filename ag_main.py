# -*- coding: utf-8 -*-
"""ag_main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IyHp0rvKKYas32NSrBqYd0obwfruR11J

# Import packages
"""

import os
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.optimizers import RMSprop

"""# Coordinate Q learning

## build Q learning
"""

class Eval_Q_Model(tf.keras.Model):
  # Evaluate Q Model 的模型架構
  def __init__(self, num_actions):
    super().__init__('mlp_q_network')
    self.layer1 = layers.Dense(10, activation='relu')
    self.logits = layers.Dense(num_actions, activation=None)

  def call(self, inputs):
    x = tf.convert_to_tensor(inputs)
    layer1 = self.layer1(x)
    logits = self.logits(layer1)
    return logits

class Target_Q_Model(tf.keras.Model):
  # Target Q Model 的模型架構(不更新layer權重)
  def __init__(self, num_actions):
    super().__init__('mlp_q_network_1')
    self.layer1 = layers.Dense(10, trainable=False, activation='relu')
    self.logits = layers.Dense(num_actions, trainable=False, activation=None)

  def call(self, inputs):
    x = tf.convert_to_tensor(inputs)
    layer1 = self.layer1(x)
    logits = self.logits(layer1)
    return logits

class DeepQNetwork:
  def __init__(self, n_actions, n_features, eval_model, target_model):
    # 利用dic儲存參數
    self.params = {
        'n_actions': n_actions,
        'n_features': n_features,
        'learning_rate': 0.01,
        'reward_decay': 0.9,
        'e_greedy': 0.9,
        'replace_target_iter': 300,
        'memory_size': 500,
        'batch_size': 32,
        'e_greedy_increment': None
        }

    # total learning step
    self.learn_step_counter = 0

    # initialize zero memory [s, a, r, s_] = [memory_size, n_features, n_features_reward, ]
    self.epsilon = 0 if self.params['e_greedy_increment'] is not None else self.params['e_greedy']
    temp = (self.params['memory_size'], self.params['n_features']*2+2)
    self.memory = np.zeros(temp)

    self.eval_model = eval_model
    self.target_model = target_model

    self.eval_model.compile(
        optimizer=RMSprop(learning_rate=self.params['learning_rate']),
        loss='mse'
        )
    self.cost_his = []

  def store_transition(self, s, a, r, s_):
    if not hasattr(self, 'memory_counter'):
      self.memory_counter = 0

    transition = np.hstack((s, [a, r], s_))

    # replace the old memory with new memory
    index = self.memory_counter % self.params['memory_size']
    self.memory[index, :] = transition

    self.memory_counter += 1

  def choose_action(self, observation):
    # to have batch dimension when feed into tf placeholder
    observation = observation[np.newaxis, :]

    if np.random.uniform() < self.epsilon:
    # forward feed the observation and get q value for every actions
      actions_value = self.eval_model.predict(observation)
      print(actions_value)
      action = np.argmax(actions_value)
    else:
      action = np.random.randint(0, self.params['n_actions'])
    return action

  def learn(self):
    # sample batch memory from all memory
    if self.memory_counter > self.params['memory_size']:
      sample_index = np.random.choice(self.params['memory_size'], size=self.params['batch_size'])
    else:
      sample_index = np.random.choice(self.memory_counter, size=self.params['batch_size'])

    batch_memory = self.memory[sample_index, :]

    q_next = self.target_model.predict(batch_memory[:, -self.params['n_features']:])
    q_eval = self.eval_model.predict(batch_memory[:, :self.params['n_features']])

    # change q_target w.r.t q_eval's action
    q_target = q_eval.copy()

    batch_index = np.arange(self.params['batch_size'], dtype=np.int32)
    eval_act_index = batch_memory[:, self.params['n_features']].astype(int)
    reward = batch_memory[:, self.params['n_features'] + 1]

    q_target[batch_index, eval_act_index] = reward + self.params['reward_decay'] * np.max(q_next, axis=1)

    # check to replace target parameters
    if self.learn_step_counter % self.params['replace_target_iter'] == 0:
      for eval_layer, target_layer in zip(self.eval_model.layers, self.target_model.layers):
        target_layer.set_weights(eval_layer.get_weights())
      print('\ntarget_params_replaced\n')

    # train eval network
    self.cost = self.eval_model.train_on_batch(batch_memory[:, :self.params['n_features']], q_target)
    self.cost_his.append(self.cost)

    # increasing epsilon
    self.epsilon = self.epsilon + self.params['e_greedy_increment'] if self.epsilon < self.params['e_greedy'] else self.params['e_greedy']
    self.learn_step_counter += 1

  def plot_cost(self):
    plt.plot(np.arange(len(self.cost_his)), self.cost_his)
    plt.ylabel('Cost')
    plt.xlabel('training steps')
    plt.show()

def run_maze():
  step = 0
  for episode in range(300):
    # initial observation
    observation = env.reset()

    while True:
      # fresh env
      env.render()
      # RL choose action based on observation
      action = RL.choose_action(observation)
      # RL take action and get next observation and reward
      observation_, reward, done = env.step(action)
      RL.store_transition(observation, action, reward, observation_)
      if (step > 200) and (step % 5 == 0):
        RL.learn()
      # swap observation
      observation = observation_
      # break while loop when end of this episode
      if done:
        break
      step += 1
  # end of game
  print('game over')
  env.destroy()

"""## build enviroment"""

class enviroment:
	def __init__(self):
		self.env = []
		self.wall = []

	def initial(self, setini):
		#	setini[0]：環境長	setini[1]：環境寬
		self.env = np.full((setini[0]+2, setini[1]+2), False, dtype=bool)
		self.wall = np.full((setini[0]+2, setini[1]+2), False, dtype=bool)

		#	包邊 w
		for j in range(setini[1]+2):
			self.wall[0][j] = 1
			self.wall[setini[0]+1][j] = 1
		for i in range(setini[0]+2):
			self.wall[i][0] = 1
			self.wall[i][setini[1]+1] = 1

	def set_wall(self, wall):
		for i in wall:
			if( i[1]>=np.size(self.env,1) or i[0]>=np.size(self.env,0) ): print("error",i)
			else:	self.wall[ i[1]+1 ][ i[0]+1 ] = 1

	def play(self):
		for i in range(np.size(self.env,0)):
			for j in range(np.size(self.env,1)):
				if(self.wall[i][j]):	print("w", end=" ")
				elif(self.env[i][j]):	print("■", end=" ")
				else:	print("□", end=" ")
			print("")

def agent_play_env(x, y, env):
	x_bound = np.size(env.env,1)
	y_bound = np.size(env.env,0)
	for i in range(-1,2,1):
		for j in range(-1,2,1):
			if( (j+x+1)>=x_bound or (i+y+1)>=y_bound ): print("error",i+x,j+j, end=" ")
			elif(i==0 and j==0):	print("a", end=" ")
			elif(env.wall[y+i+1][x+j+1]):	print("w", end=" ")
			elif(env.env[y+i+1][x+j+1]):	print("■", end=" ")
			else:	print("□", end=" ")
		print("")

def agent_play_env_full(x, y, env):
  print(x,y)
  x_bound = np.size(env.env,1)
  y_bound = np.size(env.env,0)
  for i in range(np.size(env.env,0)):
    for j in range(np.size(env.env,1)):
      if(i==y+1 and j==x+1):	print("a", end=" ")
      elif(env.wall[i][j]):	print("w", end=" ")
      elif(env.env[i][j]):	print("■", end=" ")
      else:	print("□", end=" ")
    print("")

def agent_play_env(x, y, env):
  x_bound = np.size(env.env,1)
  y_bound = np.size(env.env,0)
  for i in range(-1,2,1):
    for j in range(-1,2,1):
      if( (j+x+1)>=x_bound or (i+y+1)>=y_bound ): print("error",i+x,j+j, end=" ")
      elif(i==0 and j==0):  print("a", end=" ")
      elif(env.wall[y+i+1][x+j+1]):	print("w", end=" ")
      elif(env.env[y+i+1][x+j+1]):  print("■", end=" ")
      else:	print("□", end=" ")
    print("")

def agent_get_env(x, y, env):
  state =  np.zeros(9)
  for i in range(-1,2,1):
    for j in range(-1,2,1):
      if(i==0 and j==0):  pass
      elif(env.wall[y+i+1][x+j+1]): state[(j+1)+3*(i+1)] = 1
      elif(env.env[y+i+1][x+j+1]):  state[(j+1)+3*(i+1)] = 2
      else:	state[(j+1)+3*(i+1)] = 0
  return state

def agent_wall_test(x, y, env):
  x_bound = np.size(env.env,1)
  y_bound = np.size(env.env,0)
  if( (x+1)>=x_bound or (y+1)>=y_bound ): return 2
  elif(env.wall[y+1][x+1]==1):	return 1
  else:	return 0

class agent:
  def __init__(self):
    self.locx = -1
    self.locy = -1
    self.flag = 0
    self.wall_error = 0
    self.n_actions = 2
    self.n_features = np.zeros(9)
    self.env = 0

  def initial(self, env):
    self.env = env
    self.locx = 0
    self.locy = 0

  def set_loc(self, y, x):
    flag = agent_wall_test(x, y, self.env)
    if(flag==1):  print("wall_error")
    elif(flag==2):  print("bound_error")
    else:
      self.locx = x
      self.locy = y

  def loc_test(self, y, x):
    flag = agent_wall_test(x, y, self.env)
    if(flag==1):
      print("wall_error")
    elif(flag==2):	
      print("bound_error")
    return flag

  def go_right(self):
    self.locx += 1

  def go_left(self):
    self.locx -= 1

  def go_up(self):
    self.locy -= 1

  def go_down(self):
    self.locy += 1

  def go(self,do):
    if(do==1):	
      flag = self.loc_test(self.locy-1, self.locx)
      if(flag==0):  self.go_up()
    elif(do==2):
      flag = self.loc_test(self.locy+1, self.locx)
      if(flag==0):  self.go_down()
    elif(do==3):
      flag = self.loc_test(self.locy, self.locx-1)
      if(flag==0):  self.go_left()
    elif(do==4):
      flag = self.loc_test(self.locy, self.locx+1)
      if(flag==0):  self.go_right()
    self.flag = flag

  def step(self,action):
    self.go(action)

    # reward設計
    if(self.flag==1): reward = -1
    elif(self.flag==2):
      self.wall_error += 1
      reward = -1*(self.wall_error if self.wall_error<4 else 4)
    else: reward = 0.1 

    # 取得下一階段state
    state = agent_get_env(self.locx, self.locy, self.env)

    # 是否end game
    if(self.wall_error==20):  done = True

    return self.flag, reward, done

  def play(self):
    print("Location：", self.locx, self.locy)
    agent_play_env(self.locx, self.locy, self.env)

  def full_play(self):
    print("Location：", self.locx, self.locy)
    agent_play_env_full(self.locx, self.locy, self.env)

"""## run set"""

def run_cleaner():
  step = 0
  for episode in range(300):
    # RL choose action based on observation
    action = RL.choose_action(observation)
    # RL take action and get next observation and reward
    observation_, reward, done = ag.step(action)
    RL.store_transition(observation, action, reward, observation_)
    if (step > 200) and (step % 5 == 0):
      RL.learn()
    # swap observation
    observation = observation_
    # break while loop when end of this episode
    if done:
      break
    step += 1
  # end of game
  print('over')

"""## main"""

env = enviroment()
env.initial([10,10])
wall = np.array( [ [5, 0], [5, 1] ] )
env.set_wall( wall )

ag = agent()
ag.initial(env)
ag.set_loc(0, 0)
ag.n_features = agent_get_env(ag.locx, ag.locy, ag.env)


eval_model = Eval_Q_Model(num_actions=4)
target_model = Target_Q_Model(num_actions=4)
RL = DeepQNetwork(ag.n_actions, 4, eval_model, target_model)

